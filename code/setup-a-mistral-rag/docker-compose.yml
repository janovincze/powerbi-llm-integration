version: '3.8'

services:
  # Vector database for RAG
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334

  # Mistral inference server (vLLM)
  mistral:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: >
      --model mistralai/Mistral-7B-Instruct-v0.1
      --enable-lora
      --lora-modules sql-adapter=/models/mistral-7b-sql-lora
      --max-model-len 8192
      --gpu-memory-utilization 0.9
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # RAG API server
  rag-api:
    build:
      context: ./inference
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - MISTRAL_URL=http://mistral:8000
      - CONFLUENCE_URL=${CONFLUENCE_URL}
      - CONFLUENCE_USER=${CONFLUENCE_USER}
      - CONFLUENCE_API_KEY=${CONFLUENCE_API_KEY}
    depends_on:
      - qdrant
      - mistral

volumes:
  qdrant_data:
  huggingface_cache:
